{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, struct, torch, h5py\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from chofer_torchex.nn import SLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPersistenceDiagram(file, dim):\n",
    "    \n",
    "    birth = np.empty(0, dtype = float)\n",
    "    death = np.empty(0, dtype = float)\n",
    "    \n",
    "    file.read(24)\n",
    "    \n",
    "    for s in iter(lambda: file.read(24), b''):\n",
    "        \n",
    "        d = int(struct.unpack('<q', s[:8])[0])\n",
    "        \n",
    "        if d == dim:\n",
    "            \n",
    "            birth = np.append(birth, struct.unpack('<d', s[8:16])[0])\n",
    "            death = np.append(death, struct.unpack('<d', s[-8:])[0])\n",
    "    \n",
    "    return [birth, death]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PHdata(data.Dataset):\n",
    "    \n",
    "    def __init__(self, totnum, validate = False, nu = 1.0):\n",
    "        \n",
    "        self.nu = nu\n",
    "        self.dset = h5py.File('./PersistenceDiagrams/jetPDs_Mass60-95_pT250-300_R1.25_Pix25.hdf5','r')\n",
    "        self.totnum = totnum\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.totnum\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #if validate: idx += self.totnum\n",
    "            \n",
    "        label = int(self.dset['signal'][idx])\n",
    "        pd_birth = self.dset['pd_birth'][idx]\n",
    "        pd_death = self.dset['pd_death'][idx]\n",
    "        \n",
    "        x = (pd_birth+pd_death)/np.sqrt(2)\n",
    "        y = (-pd_birth+pd_death)/np.sqrt(2)\n",
    "        \n",
    "        #for i, bo in enumerate(y < self.nu):    \n",
    "        #    if bo: \n",
    "        #        y[i] = np.log(y[i]/self.nu) + self.nu\n",
    "        \n",
    "        mset = torch.FloatTensor(list(zip(x, y)))\n",
    "        \n",
    "        return [mset, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        sharpness = torch.ones(100, 2)*0.01\n",
    "        self.input = SLayer(100, 2, sharpness_init = sharpness)\n",
    "        self.hidden1 = torch.nn.Linear(100, 70)\n",
    "        self.hidden2 = torch.nn.Linear(70, 40)\n",
    "        self.hidden3 = torch.nn.Linear(40, 10)\n",
    "        self.hidden4 = torch.nn.Linear(10, 5)\n",
    "        self.output = torch.nn.Linear(5, 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, mset):\n",
    "        x = self.input(mset)\n",
    "        \n",
    "#        for i, xx in enumerate(x):\n",
    "#            x[i] = xx / len(mset[i])\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.relu(self.hidden3(x))\n",
    "        x = self.relu(self.hidden4(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycollate_fn(batch): \n",
    "    \n",
    "    batch_input = []\n",
    "    batch_label = []\n",
    "    \n",
    "    for x in batch:\n",
    "        \n",
    "        batch_input.append(x[0])\n",
    "        batch_label.append(x[1])\n",
    "    \n",
    "    return (batch_input, torch.LongTensor(batch_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, totnum):\n",
    "    \n",
    "    model.load_state_dict(torch.load('./TrainedModels/trained_params.dat'))\n",
    "    \n",
    "    dataset = PHdata(totnum, validate = True)\n",
    "    loader = data.DataLoader(dataset, 100, shuffle = False, collate_fn = mycollate_fn)\n",
    "    \n",
    "    csv_file = open('./Statistics/t-statistic.csv', mode = 'w')\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    for i, (batch_input, labels) in enumerate(loader):\n",
    "        outputs = model(batch_input)   \n",
    "        outputs = outputs.cpu()\n",
    "        outputs = outputs.detach().numpy()\n",
    "    \n",
    "        for j in range(len(outputs)):\n",
    "            val = np.exp(outputs[j])\n",
    "            csv_writer.writerow([val[1]/(val[0]+val[1])])\n",
    "    \n",
    "    csv_file.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batchsize, num_epoch, totnum = 2000, lr = 0.005, momentum = 0.9, fn = mycollate_fn):\n",
    "    \n",
    "    dataset = PHdata(totnum)\n",
    "    loader = data.DataLoader(dataset, batchsize, shuffle = True, collate_fn = fn)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr, momentum)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "    \n",
    "        for i, (batch_input, labels) in enumerate(loader):\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(batch_input)\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "    \n",
    "            print ('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epoch, loss.item() ))\n",
    "            \n",
    "    torch.save(model.state_dict(), './TrainedModels/trained_params.dat')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.7329\n",
      "Epoch [1/200], Loss: 0.7327\n",
      "Epoch [1/200], Loss: 0.7322\n",
      "Epoch [1/200], Loss: 0.7208\n",
      "Epoch [2/200], Loss: 0.7288\n",
      "Epoch [2/200], Loss: 0.6991\n",
      "Epoch [2/200], Loss: 0.7164\n",
      "Epoch [2/200], Loss: 0.7180\n",
      "Epoch [3/200], Loss: 0.6992\n",
      "Epoch [3/200], Loss: 0.7041\n",
      "Epoch [3/200], Loss: 0.7192\n",
      "Epoch [3/200], Loss: 0.7108\n",
      "Epoch [4/200], Loss: 0.6969\n",
      "Epoch [4/200], Loss: 0.7061\n",
      "Epoch [4/200], Loss: 0.7151\n",
      "Epoch [4/200], Loss: 0.7026\n",
      "Epoch [5/200], Loss: 0.6991\n",
      "Epoch [5/200], Loss: 0.6954\n",
      "Epoch [5/200], Loss: 0.7069\n",
      "Epoch [5/200], Loss: 0.7107\n",
      "Epoch [6/200], Loss: 0.6949\n",
      "Epoch [6/200], Loss: 0.7005\n",
      "Epoch [6/200], Loss: 0.7031\n",
      "Epoch [6/200], Loss: 0.7060\n",
      "Epoch [7/200], Loss: 0.6914\n",
      "Epoch [7/200], Loss: 0.7007\n",
      "Epoch [7/200], Loss: 0.7004\n",
      "Epoch [7/200], Loss: 0.7053\n",
      "Epoch [8/200], Loss: 0.7064\n",
      "Epoch [8/200], Loss: 0.6954\n",
      "Epoch [8/200], Loss: 0.6971\n",
      "Epoch [8/200], Loss: 0.6941\n",
      "Epoch [9/200], Loss: 0.6943\n",
      "Epoch [9/200], Loss: 0.7032\n",
      "Epoch [9/200], Loss: 0.6996\n",
      "Epoch [9/200], Loss: 0.6907\n",
      "Epoch [10/200], Loss: 0.6956\n",
      "Epoch [10/200], Loss: 0.6983\n",
      "Epoch [10/200], Loss: 0.6958\n",
      "Epoch [10/200], Loss: 0.6941\n",
      "Epoch [11/200], Loss: 0.6949\n",
      "Epoch [11/200], Loss: 0.6910\n",
      "Epoch [11/200], Loss: 0.6970\n",
      "Epoch [11/200], Loss: 0.6975\n",
      "Epoch [12/200], Loss: 0.6949\n",
      "Epoch [12/200], Loss: 0.6972\n",
      "Epoch [12/200], Loss: 0.6965\n",
      "Epoch [12/200], Loss: 0.6898\n",
      "Epoch [13/200], Loss: 0.6947\n",
      "Epoch [13/200], Loss: 0.6916\n",
      "Epoch [13/200], Loss: 0.6918\n",
      "Epoch [13/200], Loss: 0.6979\n",
      "Epoch [14/200], Loss: 0.6961\n",
      "Epoch [14/200], Loss: 0.6938\n",
      "Epoch [14/200], Loss: 0.6926\n",
      "Epoch [14/200], Loss: 0.6922\n",
      "Epoch [15/200], Loss: 0.6923\n",
      "Epoch [15/200], Loss: 0.6956\n",
      "Epoch [15/200], Loss: 0.6913\n",
      "Epoch [15/200], Loss: 0.6939\n",
      "Epoch [16/200], Loss: 0.6935\n",
      "Epoch [16/200], Loss: 0.6937\n",
      "Epoch [16/200], Loss: 0.6915\n",
      "Epoch [16/200], Loss: 0.6932\n",
      "Epoch [17/200], Loss: 0.6933\n",
      "Epoch [17/200], Loss: 0.6919\n",
      "Epoch [17/200], Loss: 0.6937\n",
      "Epoch [17/200], Loss: 0.6920\n",
      "Epoch [18/200], Loss: 0.6925\n",
      "Epoch [18/200], Loss: 0.6932\n",
      "Epoch [18/200], Loss: 0.6912\n",
      "Epoch [18/200], Loss: 0.6930\n",
      "Epoch [19/200], Loss: 0.6918\n",
      "Epoch [19/200], Loss: 0.6927\n",
      "Epoch [19/200], Loss: 0.6925\n",
      "Epoch [19/200], Loss: 0.6921\n",
      "Epoch [20/200], Loss: 0.6929\n",
      "Epoch [20/200], Loss: 0.6923\n",
      "Epoch [20/200], Loss: 0.6914\n",
      "Epoch [20/200], Loss: 0.6917\n",
      "Epoch [21/200], Loss: 0.6918\n",
      "Epoch [21/200], Loss: 0.6927\n",
      "Epoch [21/200], Loss: 0.6918\n",
      "Epoch [21/200], Loss: 0.6912\n",
      "Epoch [22/200], Loss: 0.6922\n",
      "Epoch [22/200], Loss: 0.6920\n",
      "Epoch [22/200], Loss: 0.6922\n",
      "Epoch [22/200], Loss: 0.6902\n",
      "Epoch [23/200], Loss: 0.6911\n",
      "Epoch [23/200], Loss: 0.6915\n",
      "Epoch [23/200], Loss: 0.6913\n",
      "Epoch [23/200], Loss: 0.6917\n",
      "Epoch [24/200], Loss: 0.6908\n",
      "Epoch [24/200], Loss: 0.6912\n",
      "Epoch [24/200], Loss: 0.6917\n",
      "Epoch [24/200], Loss: 0.6911\n",
      "Epoch [25/200], Loss: 0.6903\n",
      "Epoch [25/200], Loss: 0.6907\n",
      "Epoch [25/200], Loss: 0.6909\n",
      "Epoch [25/200], Loss: 0.6920\n",
      "Epoch [26/200], Loss: 0.6913\n",
      "Epoch [26/200], Loss: 0.6911\n",
      "Epoch [26/200], Loss: 0.6905\n",
      "Epoch [26/200], Loss: 0.6901\n",
      "Epoch [27/200], Loss: 0.6898\n",
      "Epoch [27/200], Loss: 0.6909\n",
      "Epoch [27/200], Loss: 0.6904\n",
      "Epoch [27/200], Loss: 0.6906\n",
      "Epoch [28/200], Loss: 0.6909\n",
      "Epoch [28/200], Loss: 0.6907\n",
      "Epoch [28/200], Loss: 0.6900\n",
      "Epoch [28/200], Loss: 0.6891\n",
      "Epoch [29/200], Loss: 0.6897\n",
      "Epoch [29/200], Loss: 0.6908\n",
      "Epoch [29/200], Loss: 0.6898\n",
      "Epoch [29/200], Loss: 0.6894\n",
      "Epoch [30/200], Loss: 0.6893\n",
      "Epoch [30/200], Loss: 0.6901\n",
      "Epoch [30/200], Loss: 0.6905\n",
      "Epoch [30/200], Loss: 0.6885\n",
      "Epoch [31/200], Loss: 0.6893\n",
      "Epoch [31/200], Loss: 0.6891\n",
      "Epoch [31/200], Loss: 0.6896\n",
      "Epoch [31/200], Loss: 0.6892\n",
      "Epoch [32/200], Loss: 0.6898\n",
      "Epoch [32/200], Loss: 0.6884\n",
      "Epoch [32/200], Loss: 0.6886\n",
      "Epoch [32/200], Loss: 0.6890\n",
      "Epoch [33/200], Loss: 0.6885\n",
      "Epoch [33/200], Loss: 0.6890\n",
      "Epoch [33/200], Loss: 0.6891\n",
      "Epoch [33/200], Loss: 0.6879\n",
      "Epoch [34/200], Loss: 0.6885\n",
      "Epoch [34/200], Loss: 0.6880\n",
      "Epoch [34/200], Loss: 0.6884\n",
      "Epoch [34/200], Loss: 0.6880\n",
      "Epoch [35/200], Loss: 0.6888\n",
      "Epoch [35/200], Loss: 0.6875\n",
      "Epoch [35/200], Loss: 0.6873\n",
      "Epoch [35/200], Loss: 0.6876\n",
      "Epoch [36/200], Loss: 0.6874\n",
      "Epoch [36/200], Loss: 0.6870\n",
      "Epoch [36/200], Loss: 0.6870\n",
      "Epoch [36/200], Loss: 0.6880\n",
      "Epoch [37/200], Loss: 0.6875\n",
      "Epoch [37/200], Loss: 0.6869\n",
      "Epoch [37/200], Loss: 0.6865\n",
      "Epoch [37/200], Loss: 0.6864\n",
      "Epoch [38/200], Loss: 0.6860\n",
      "Epoch [38/200], Loss: 0.6869\n",
      "Epoch [38/200], Loss: 0.6853\n",
      "Epoch [38/200], Loss: 0.6870\n",
      "Epoch [39/200], Loss: 0.6845\n",
      "Epoch [39/200], Loss: 0.6869\n",
      "Epoch [39/200], Loss: 0.6856\n",
      "Epoch [39/200], Loss: 0.6855\n",
      "Epoch [40/200], Loss: 0.6849\n",
      "Epoch [40/200], Loss: 0.6856\n",
      "Epoch [40/200], Loss: 0.6856\n",
      "Epoch [40/200], Loss: 0.6839\n",
      "Epoch [41/200], Loss: 0.6843\n",
      "Epoch [41/200], Loss: 0.6840\n",
      "Epoch [41/200], Loss: 0.6838\n",
      "Epoch [41/200], Loss: 0.6851\n",
      "Epoch [42/200], Loss: 0.6842\n",
      "Epoch [42/200], Loss: 0.6864\n",
      "Epoch [42/200], Loss: 0.6808\n",
      "Epoch [42/200], Loss: 0.6826\n",
      "Epoch [43/200], Loss: 0.6831\n",
      "Epoch [43/200], Loss: 0.6824\n",
      "Epoch [43/200], Loss: 0.6830\n",
      "Epoch [43/200], Loss: 0.6825\n",
      "Epoch [44/200], Loss: 0.6820\n",
      "Epoch [44/200], Loss: 0.6796\n",
      "Epoch [44/200], Loss: 0.6825\n",
      "Epoch [44/200], Loss: 0.6835\n",
      "Epoch [45/200], Loss: 0.6823\n",
      "Epoch [45/200], Loss: 0.6791\n",
      "Epoch [45/200], Loss: 0.6818\n",
      "Epoch [45/200], Loss: 0.6804\n",
      "Epoch [46/200], Loss: 0.6806\n",
      "Epoch [46/200], Loss: 0.6786\n",
      "Epoch [46/200], Loss: 0.6821\n",
      "Epoch [46/200], Loss: 0.6783\n",
      "Epoch [47/200], Loss: 0.6788\n",
      "Epoch [47/200], Loss: 0.6807\n",
      "Epoch [47/200], Loss: 0.6773\n",
      "Epoch [47/200], Loss: 0.6782\n",
      "Epoch [48/200], Loss: 0.6795\n",
      "Epoch [48/200], Loss: 0.6775\n",
      "Epoch [48/200], Loss: 0.6766\n",
      "Epoch [48/200], Loss: 0.6773\n",
      "Epoch [49/200], Loss: 0.6786\n",
      "Epoch [49/200], Loss: 0.6743\n",
      "Epoch [49/200], Loss: 0.6756\n",
      "Epoch [49/200], Loss: 0.6782\n",
      "Epoch [50/200], Loss: 0.6772\n",
      "Epoch [50/200], Loss: 0.6755\n",
      "Epoch [50/200], Loss: 0.6760\n",
      "Epoch [50/200], Loss: 0.6740\n",
      "Epoch [51/200], Loss: 0.6759\n",
      "Epoch [51/200], Loss: 0.6765\n",
      "Epoch [51/200], Loss: 0.6720\n",
      "Epoch [51/200], Loss: 0.6743\n",
      "Epoch [52/200], Loss: 0.6762\n",
      "Epoch [52/200], Loss: 0.6727\n",
      "Epoch [52/200], Loss: 0.6746\n",
      "Epoch [52/200], Loss: 0.6710\n",
      "Epoch [53/200], Loss: 0.6747\n",
      "Epoch [53/200], Loss: 0.6743\n",
      "Epoch [53/200], Loss: 0.6722\n",
      "Epoch [53/200], Loss: 0.6692\n",
      "Epoch [54/200], Loss: 0.6732\n",
      "Epoch [54/200], Loss: 0.6664\n",
      "Epoch [54/200], Loss: 0.6738\n",
      "Epoch [54/200], Loss: 0.6729\n",
      "Epoch [55/200], Loss: 0.6721\n",
      "Epoch [55/200], Loss: 0.6714\n",
      "Epoch [55/200], Loss: 0.6675\n",
      "Epoch [55/200], Loss: 0.6710\n",
      "Epoch [56/200], Loss: 0.6678\n",
      "Epoch [56/200], Loss: 0.6685\n",
      "Epoch [56/200], Loss: 0.6697\n",
      "Epoch [56/200], Loss: 0.6719\n",
      "Epoch [57/200], Loss: 0.6700\n",
      "Epoch [57/200], Loss: 0.6662\n",
      "Epoch [57/200], Loss: 0.6655\n",
      "Epoch [57/200], Loss: 0.6716\n",
      "Epoch [58/200], Loss: 0.6716\n",
      "Epoch [58/200], Loss: 0.6659\n",
      "Epoch [58/200], Loss: 0.6627\n",
      "Epoch [58/200], Loss: 0.6685\n",
      "Epoch [59/200], Loss: 0.6672\n",
      "Epoch [59/200], Loss: 0.6705\n",
      "Epoch [59/200], Loss: 0.6606\n",
      "Epoch [59/200], Loss: 0.6660\n",
      "Epoch [60/200], Loss: 0.6636\n",
      "Epoch [60/200], Loss: 0.6676\n",
      "Epoch [60/200], Loss: 0.6639\n",
      "Epoch [60/200], Loss: 0.6647\n",
      "Epoch [61/200], Loss: 0.6668\n",
      "Epoch [61/200], Loss: 0.6646\n",
      "Epoch [61/200], Loss: 0.6601\n",
      "Epoch [61/200], Loss: 0.6640\n",
      "Epoch [62/200], Loss: 0.6609\n",
      "Epoch [62/200], Loss: 0.6624\n",
      "Epoch [62/200], Loss: 0.6669\n",
      "Epoch [62/200], Loss: 0.6609\n",
      "Epoch [63/200], Loss: 0.6563\n",
      "Epoch [63/200], Loss: 0.6625\n",
      "Epoch [63/200], Loss: 0.6646\n",
      "Epoch [63/200], Loss: 0.6626\n",
      "Epoch [64/200], Loss: 0.6600\n",
      "Epoch [64/200], Loss: 0.6628\n",
      "Epoch [64/200], Loss: 0.6605\n",
      "Epoch [64/200], Loss: 0.6578\n",
      "Epoch [65/200], Loss: 0.6599\n",
      "Epoch [65/200], Loss: 0.6587\n",
      "Epoch [65/200], Loss: 0.6574\n",
      "Epoch [65/200], Loss: 0.6601\n",
      "Epoch [66/200], Loss: 0.6510\n",
      "Epoch [66/200], Loss: 0.6571\n",
      "Epoch [66/200], Loss: 0.6608\n",
      "Epoch [66/200], Loss: 0.6634\n",
      "Epoch [67/200], Loss: 0.6605\n",
      "Epoch [67/200], Loss: 0.6604\n",
      "Epoch [67/200], Loss: 0.6527\n",
      "Epoch [67/200], Loss: 0.6532\n",
      "Epoch [68/200], Loss: 0.6573\n",
      "Epoch [68/200], Loss: 0.6466\n",
      "Epoch [68/200], Loss: 0.6553\n",
      "Epoch [68/200], Loss: 0.6624\n",
      "Epoch [69/200], Loss: 0.6620\n",
      "Epoch [69/200], Loss: 0.6559\n",
      "Epoch [69/200], Loss: 0.6540\n",
      "Epoch [69/200], Loss: 0.6442\n",
      "Epoch [70/200], Loss: 0.6519\n",
      "Epoch [70/200], Loss: 0.6543\n",
      "Epoch [70/200], Loss: 0.6488\n",
      "Epoch [70/200], Loss: 0.6557\n",
      "Epoch [71/200], Loss: 0.6566\n",
      "Epoch [71/200], Loss: 0.6497\n",
      "Epoch [71/200], Loss: 0.6545\n",
      "Epoch [71/200], Loss: 0.6455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/200], Loss: 0.6471\n",
      "Epoch [72/200], Loss: 0.6540\n",
      "Epoch [72/200], Loss: 0.6498\n",
      "Epoch [72/200], Loss: 0.6525\n",
      "Epoch [73/200], Loss: 0.6574\n",
      "Epoch [73/200], Loss: 0.6470\n",
      "Epoch [73/200], Loss: 0.6445\n",
      "Epoch [73/200], Loss: 0.6483\n",
      "Epoch [74/200], Loss: 0.6517\n",
      "Epoch [74/200], Loss: 0.6508\n",
      "Epoch [74/200], Loss: 0.6462\n",
      "Epoch [74/200], Loss: 0.6411\n",
      "Epoch [75/200], Loss: 0.6480\n",
      "Epoch [75/200], Loss: 0.6471\n",
      "Epoch [75/200], Loss: 0.6439\n",
      "Epoch [75/200], Loss: 0.6451\n",
      "Epoch [76/200], Loss: 0.6359\n",
      "Epoch [76/200], Loss: 0.6516\n",
      "Epoch [76/200], Loss: 0.6468\n",
      "Epoch [76/200], Loss: 0.6428\n",
      "Epoch [77/200], Loss: 0.6443\n",
      "Epoch [77/200], Loss: 0.6375\n",
      "Epoch [77/200], Loss: 0.6389\n",
      "Epoch [77/200], Loss: 0.6517\n",
      "Epoch [78/200], Loss: 0.6453\n",
      "Epoch [78/200], Loss: 0.6389\n",
      "Epoch [78/200], Loss: 0.6369\n",
      "Epoch [78/200], Loss: 0.6439\n",
      "Epoch [79/200], Loss: 0.6383\n",
      "Epoch [79/200], Loss: 0.6481\n",
      "Epoch [79/200], Loss: 0.6319\n",
      "Epoch [79/200], Loss: 0.6408\n",
      "Epoch [80/200], Loss: 0.6382\n",
      "Epoch [80/200], Loss: 0.6451\n",
      "Epoch [80/200], Loss: 0.6362\n",
      "Epoch [80/200], Loss: 0.6327\n",
      "Epoch [81/200], Loss: 0.6359\n",
      "Epoch [81/200], Loss: 0.6342\n",
      "Epoch [81/200], Loss: 0.6323\n",
      "Epoch [81/200], Loss: 0.6400\n",
      "Epoch [82/200], Loss: 0.6318\n",
      "Epoch [82/200], Loss: 0.6402\n",
      "Epoch [82/200], Loss: 0.6309\n",
      "Epoch [82/200], Loss: 0.6311\n",
      "Epoch [83/200], Loss: 0.6358\n",
      "Epoch [83/200], Loss: 0.6238\n",
      "Epoch [83/200], Loss: 0.6360\n",
      "Epoch [83/200], Loss: 0.6294\n",
      "Epoch [84/200], Loss: 0.6280\n",
      "Epoch [84/200], Loss: 0.6299\n",
      "Epoch [84/200], Loss: 0.6274\n",
      "Epoch [84/200], Loss: 0.6334\n",
      "Epoch [85/200], Loss: 0.6260\n",
      "Epoch [85/200], Loss: 0.6304\n",
      "Epoch [85/200], Loss: 0.6252\n",
      "Epoch [85/200], Loss: 0.6265\n",
      "Epoch [86/200], Loss: 0.6229\n",
      "Epoch [86/200], Loss: 0.6215\n",
      "Epoch [86/200], Loss: 0.6249\n",
      "Epoch [86/200], Loss: 0.6325\n",
      "Epoch [87/200], Loss: 0.6282\n",
      "Epoch [87/200], Loss: 0.6259\n",
      "Epoch [87/200], Loss: 0.6172\n",
      "Epoch [87/200], Loss: 0.6211\n",
      "Epoch [88/200], Loss: 0.6257\n",
      "Epoch [88/200], Loss: 0.6285\n",
      "Epoch [88/200], Loss: 0.6123\n",
      "Epoch [88/200], Loss: 0.6201\n",
      "Epoch [89/200], Loss: 0.6228\n",
      "Epoch [89/200], Loss: 0.6190\n",
      "Epoch [89/200], Loss: 0.6246\n",
      "Epoch [89/200], Loss: 0.6093\n",
      "Epoch [90/200], Loss: 0.6110\n",
      "Epoch [90/200], Loss: 0.6103\n",
      "Epoch [90/200], Loss: 0.6281\n",
      "Epoch [90/200], Loss: 0.6183\n",
      "Epoch [91/200], Loss: 0.6184\n",
      "Epoch [91/200], Loss: 0.6149\n",
      "Epoch [91/200], Loss: 0.6110\n",
      "Epoch [91/200], Loss: 0.6158\n",
      "Epoch [92/200], Loss: 0.6068\n",
      "Epoch [92/200], Loss: 0.6056\n",
      "Epoch [92/200], Loss: 0.6026\n",
      "Epoch [92/200], Loss: 0.6382\n",
      "Epoch [93/200], Loss: 0.6063\n",
      "Epoch [93/200], Loss: 0.6148\n",
      "Epoch [93/200], Loss: 0.6160\n",
      "Epoch [93/200], Loss: 0.6050\n",
      "Epoch [94/200], Loss: 0.6132\n",
      "Epoch [94/200], Loss: 0.6094\n",
      "Epoch [94/200], Loss: 0.5986\n",
      "Epoch [94/200], Loss: 0.6098\n",
      "Epoch [95/200], Loss: 0.5959\n",
      "Epoch [95/200], Loss: 0.5956\n",
      "Epoch [95/200], Loss: 0.6307\n",
      "Epoch [95/200], Loss: 0.6058\n",
      "Epoch [96/200], Loss: 0.6136\n",
      "Epoch [96/200], Loss: 0.5910\n",
      "Epoch [96/200], Loss: 0.6111\n",
      "Epoch [96/200], Loss: 0.6095\n",
      "Epoch [97/200], Loss: 0.6084\n",
      "Epoch [97/200], Loss: 0.5908\n",
      "Epoch [97/200], Loss: 0.6027\n",
      "Epoch [97/200], Loss: 0.6098\n",
      "Epoch [98/200], Loss: 0.5942\n",
      "Epoch [98/200], Loss: 0.6093\n",
      "Epoch [98/200], Loss: 0.6028\n",
      "Epoch [98/200], Loss: 0.6070\n",
      "Epoch [99/200], Loss: 0.6139\n",
      "Epoch [99/200], Loss: 0.5919\n",
      "Epoch [99/200], Loss: 0.6290\n",
      "Epoch [99/200], Loss: 0.6051\n",
      "Epoch [100/200], Loss: 0.5973\n",
      "Epoch [100/200], Loss: 0.5938\n",
      "Epoch [100/200], Loss: 0.6185\n",
      "Epoch [100/200], Loss: 0.6047\n",
      "Epoch [101/200], Loss: 0.5867\n",
      "Epoch [101/200], Loss: 0.5889\n",
      "Epoch [101/200], Loss: 0.6179\n",
      "Epoch [101/200], Loss: 0.6081\n",
      "Epoch [102/200], Loss: 0.6015\n",
      "Epoch [102/200], Loss: 0.5870\n",
      "Epoch [102/200], Loss: 0.5966\n",
      "Epoch [102/200], Loss: 0.5884\n",
      "Epoch [103/200], Loss: 0.5809\n",
      "Epoch [103/200], Loss: 0.6040\n",
      "Epoch [103/200], Loss: 0.5886\n",
      "Epoch [103/200], Loss: 0.6040\n",
      "Epoch [104/200], Loss: 0.6003\n",
      "Epoch [104/200], Loss: 0.5961\n",
      "Epoch [104/200], Loss: 0.5725\n",
      "Epoch [104/200], Loss: 0.5876\n",
      "Epoch [105/200], Loss: 0.5890\n",
      "Epoch [105/200], Loss: 0.5937\n",
      "Epoch [105/200], Loss: 0.5822\n",
      "Epoch [105/200], Loss: 0.5987\n",
      "Epoch [106/200], Loss: 0.5988\n",
      "Epoch [106/200], Loss: 0.5841\n",
      "Epoch [106/200], Loss: 0.5760\n",
      "Epoch [106/200], Loss: 0.5845\n",
      "Epoch [107/200], Loss: 0.6078\n",
      "Epoch [107/200], Loss: 0.5717\n",
      "Epoch [107/200], Loss: 0.5891\n",
      "Epoch [107/200], Loss: 0.5785\n",
      "Epoch [108/200], Loss: 0.6024\n",
      "Epoch [108/200], Loss: 0.5759\n",
      "Epoch [108/200], Loss: 0.5912\n",
      "Epoch [108/200], Loss: 0.5743\n",
      "Epoch [109/200], Loss: 0.5685\n",
      "Epoch [109/200], Loss: 0.5832\n",
      "Epoch [109/200], Loss: 0.5774\n",
      "Epoch [109/200], Loss: 0.5991\n",
      "Epoch [110/200], Loss: 0.5825\n",
      "Epoch [110/200], Loss: 0.5824\n",
      "Epoch [110/200], Loss: 0.5906\n",
      "Epoch [110/200], Loss: 0.5633\n",
      "Epoch [111/200], Loss: 0.5856\n",
      "Epoch [111/200], Loss: 0.5740\n",
      "Epoch [111/200], Loss: 0.5744\n",
      "Epoch [111/200], Loss: 0.5819\n",
      "Epoch [112/200], Loss: 0.5708\n",
      "Epoch [112/200], Loss: 0.5861\n",
      "Epoch [112/200], Loss: 0.5848\n",
      "Epoch [112/200], Loss: 0.5782\n",
      "Epoch [113/200], Loss: 0.5541\n",
      "Epoch [113/200], Loss: 0.5630\n",
      "Epoch [113/200], Loss: 0.5763\n",
      "Epoch [113/200], Loss: 0.6149\n",
      "Epoch [114/200], Loss: 0.5726\n",
      "Epoch [114/200], Loss: 0.5737\n",
      "Epoch [114/200], Loss: 0.5808\n",
      "Epoch [114/200], Loss: 0.5777\n",
      "Epoch [115/200], Loss: 0.5667\n",
      "Epoch [115/200], Loss: 0.5911\n",
      "Epoch [115/200], Loss: 0.5793\n",
      "Epoch [115/200], Loss: 0.5793\n",
      "Epoch [116/200], Loss: 0.5849\n",
      "Epoch [116/200], Loss: 0.5767\n",
      "Epoch [116/200], Loss: 0.5563\n",
      "Epoch [116/200], Loss: 0.5934\n",
      "Epoch [117/200], Loss: 0.5828\n",
      "Epoch [117/200], Loss: 0.5839\n",
      "Epoch [117/200], Loss: 0.5807\n",
      "Epoch [117/200], Loss: 0.5475\n",
      "Epoch [118/200], Loss: 0.5919\n",
      "Epoch [118/200], Loss: 0.5423\n",
      "Epoch [118/200], Loss: 0.5661\n",
      "Epoch [118/200], Loss: 0.5900\n",
      "Epoch [119/200], Loss: 0.5639\n",
      "Epoch [119/200], Loss: 0.5827\n",
      "Epoch [119/200], Loss: 0.5681\n",
      "Epoch [119/200], Loss: 0.5778\n",
      "Epoch [120/200], Loss: 0.5960\n",
      "Epoch [120/200], Loss: 0.5910\n",
      "Epoch [120/200], Loss: 0.5536\n",
      "Epoch [120/200], Loss: 0.5994\n",
      "Epoch [121/200], Loss: 0.5680\n",
      "Epoch [121/200], Loss: 0.5677\n",
      "Epoch [121/200], Loss: 0.5980\n",
      "Epoch [121/200], Loss: 0.5742\n",
      "Epoch [122/200], Loss: 0.5965\n",
      "Epoch [122/200], Loss: 0.5438\n",
      "Epoch [122/200], Loss: 0.5816\n",
      "Epoch [122/200], Loss: 0.5613\n",
      "Epoch [123/200], Loss: 0.5565\n",
      "Epoch [123/200], Loss: 0.5642\n",
      "Epoch [123/200], Loss: 0.5942\n",
      "Epoch [123/200], Loss: 0.5747\n",
      "Epoch [124/200], Loss: 0.5700\n",
      "Epoch [124/200], Loss: 0.5904\n",
      "Epoch [124/200], Loss: 0.5366\n",
      "Epoch [124/200], Loss: 0.5770\n",
      "Epoch [125/200], Loss: 0.5903\n",
      "Epoch [125/200], Loss: 0.5587\n",
      "Epoch [125/200], Loss: 0.5562\n",
      "Epoch [125/200], Loss: 0.5721\n",
      "Epoch [126/200], Loss: 0.5513\n",
      "Epoch [126/200], Loss: 0.5642\n",
      "Epoch [126/200], Loss: 0.5730\n",
      "Epoch [126/200], Loss: 0.5917\n",
      "Epoch [127/200], Loss: 0.5386\n",
      "Epoch [127/200], Loss: 0.5877\n",
      "Epoch [127/200], Loss: 0.5933\n",
      "Epoch [127/200], Loss: 0.5531\n",
      "Epoch [128/200], Loss: 0.5743\n",
      "Epoch [128/200], Loss: 0.5876\n",
      "Epoch [128/200], Loss: 0.5646\n",
      "Epoch [128/200], Loss: 0.5645\n",
      "Epoch [129/200], Loss: 0.5565\n",
      "Epoch [129/200], Loss: 0.5713\n",
      "Epoch [129/200], Loss: 0.5721\n",
      "Epoch [129/200], Loss: 0.5758\n",
      "Epoch [130/200], Loss: 0.5608\n",
      "Epoch [130/200], Loss: 0.5522\n",
      "Epoch [130/200], Loss: 0.5569\n",
      "Epoch [130/200], Loss: 0.5943\n",
      "Epoch [131/200], Loss: 0.5877\n",
      "Epoch [131/200], Loss: 0.5761\n",
      "Epoch [131/200], Loss: 0.5302\n",
      "Epoch [131/200], Loss: 0.5676\n",
      "Epoch [132/200], Loss: 0.5598\n",
      "Epoch [132/200], Loss: 0.6068\n",
      "Epoch [132/200], Loss: 0.5546\n",
      "Epoch [132/200], Loss: 0.5654\n",
      "Epoch [133/200], Loss: 0.5560\n",
      "Epoch [133/200], Loss: 0.5808\n",
      "Epoch [133/200], Loss: 0.5731\n",
      "Epoch [133/200], Loss: 0.5789\n",
      "Epoch [134/200], Loss: 0.5629\n",
      "Epoch [134/200], Loss: 0.5526\n",
      "Epoch [134/200], Loss: 0.5714\n",
      "Epoch [134/200], Loss: 0.6000\n",
      "Epoch [135/200], Loss: 0.5823\n",
      "Epoch [135/200], Loss: 0.6181\n",
      "Epoch [135/200], Loss: 0.5507\n",
      "Epoch [135/200], Loss: 0.5630\n",
      "Epoch [136/200], Loss: 0.5641\n",
      "Epoch [136/200], Loss: 0.5612\n",
      "Epoch [136/200], Loss: 0.5759\n",
      "Epoch [136/200], Loss: 0.5912\n",
      "Epoch [137/200], Loss: 0.5845\n",
      "Epoch [137/200], Loss: 0.5630\n",
      "Epoch [137/200], Loss: 0.5406\n",
      "Epoch [137/200], Loss: 0.5796\n",
      "Epoch [138/200], Loss: 0.6211\n",
      "Epoch [138/200], Loss: 0.5667\n",
      "Epoch [138/200], Loss: 0.5559\n",
      "Epoch [138/200], Loss: 0.5707\n",
      "Epoch [139/200], Loss: 0.5668\n",
      "Epoch [139/200], Loss: 0.5625\n",
      "Epoch [139/200], Loss: 0.6003\n",
      "Epoch [139/200], Loss: 0.5535\n",
      "Epoch [140/200], Loss: 0.5608\n",
      "Epoch [140/200], Loss: 0.5845\n",
      "Epoch [140/200], Loss: 0.5678\n",
      "Epoch [140/200], Loss: 0.5860\n",
      "Epoch [141/200], Loss: 0.5646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [141/200], Loss: 0.5638\n",
      "Epoch [141/200], Loss: 0.5862\n",
      "Epoch [141/200], Loss: 0.5581\n",
      "Epoch [142/200], Loss: 0.5595\n",
      "Epoch [142/200], Loss: 0.5584\n",
      "Epoch [142/200], Loss: 0.5893\n",
      "Epoch [142/200], Loss: 0.5672\n",
      "Epoch [143/200], Loss: 0.5881\n",
      "Epoch [143/200], Loss: 0.5822\n",
      "Epoch [143/200], Loss: 0.5231\n",
      "Epoch [143/200], Loss: 0.5503\n",
      "Epoch [144/200], Loss: 0.5503\n",
      "Epoch [144/200], Loss: 0.5591\n",
      "Epoch [144/200], Loss: 0.5631\n",
      "Epoch [144/200], Loss: 0.5673\n",
      "Epoch [145/200], Loss: 0.5662\n",
      "Epoch [145/200], Loss: 0.5787\n",
      "Epoch [145/200], Loss: 0.5270\n",
      "Epoch [145/200], Loss: 0.5617\n",
      "Epoch [146/200], Loss: 0.5490\n",
      "Epoch [146/200], Loss: 0.6080\n",
      "Epoch [146/200], Loss: 0.5534\n",
      "Epoch [146/200], Loss: 0.5833\n",
      "Epoch [147/200], Loss: 0.5510\n",
      "Epoch [147/200], Loss: 0.5711\n",
      "Epoch [147/200], Loss: 0.5859\n",
      "Epoch [147/200], Loss: 0.5574\n",
      "Epoch [148/200], Loss: 0.5728\n",
      "Epoch [148/200], Loss: 0.5681\n",
      "Epoch [148/200], Loss: 0.5558\n",
      "Epoch [148/200], Loss: 0.5461\n",
      "Epoch [149/200], Loss: 0.5624\n",
      "Epoch [149/200], Loss: 0.5577\n",
      "Epoch [149/200], Loss: 0.5483\n",
      "Epoch [149/200], Loss: 0.5707\n",
      "Epoch [150/200], Loss: 0.5456\n",
      "Epoch [150/200], Loss: 0.5938\n",
      "Epoch [150/200], Loss: 0.5471\n",
      "Epoch [150/200], Loss: 0.5444\n",
      "Epoch [151/200], Loss: 0.5682\n",
      "Epoch [151/200], Loss: 0.5684\n",
      "Epoch [151/200], Loss: 0.5464\n",
      "Epoch [151/200], Loss: 0.5370\n",
      "Epoch [152/200], Loss: 0.5767\n",
      "Epoch [152/200], Loss: 0.5260\n",
      "Epoch [152/200], Loss: 0.5664\n",
      "Epoch [152/200], Loss: 0.5560\n",
      "Epoch [153/200], Loss: 0.5496\n",
      "Epoch [153/200], Loss: 0.5349\n",
      "Epoch [153/200], Loss: 0.5594\n",
      "Epoch [153/200], Loss: 0.5990\n",
      "Epoch [154/200], Loss: 0.5602\n",
      "Epoch [154/200], Loss: 0.5965\n",
      "Epoch [154/200], Loss: 0.5690\n",
      "Epoch [154/200], Loss: 0.5447\n",
      "Epoch [155/200], Loss: 0.5528\n",
      "Epoch [155/200], Loss: 0.5606\n",
      "Epoch [155/200], Loss: 0.5860\n",
      "Epoch [155/200], Loss: 0.5825\n",
      "Epoch [156/200], Loss: 0.5673\n",
      "Epoch [156/200], Loss: 0.5526\n",
      "Epoch [156/200], Loss: 0.5746\n",
      "Epoch [156/200], Loss: 0.5365\n",
      "Epoch [157/200], Loss: 0.5385\n",
      "Epoch [157/200], Loss: 0.5986\n",
      "Epoch [157/200], Loss: 0.5810\n",
      "Epoch [157/200], Loss: 0.5240\n",
      "Epoch [158/200], Loss: 0.5559\n",
      "Epoch [158/200], Loss: 0.5715\n",
      "Epoch [158/200], Loss: 0.5312\n",
      "Epoch [158/200], Loss: 0.5531\n",
      "Epoch [159/200], Loss: 0.5623\n",
      "Epoch [159/200], Loss: 0.5659\n",
      "Epoch [159/200], Loss: 0.5425\n",
      "Epoch [159/200], Loss: 0.5484\n",
      "Epoch [160/200], Loss: 0.5562\n",
      "Epoch [160/200], Loss: 0.5555\n",
      "Epoch [160/200], Loss: 0.5525\n",
      "Epoch [160/200], Loss: 0.5532\n",
      "Epoch [161/200], Loss: 0.5380\n",
      "Epoch [161/200], Loss: 0.5752\n",
      "Epoch [161/200], Loss: 0.5674\n",
      "Epoch [161/200], Loss: 0.5693\n",
      "Epoch [162/200], Loss: 0.5640\n",
      "Epoch [162/200], Loss: 0.5814\n",
      "Epoch [162/200], Loss: 0.5362\n",
      "Epoch [162/200], Loss: 0.5345\n",
      "Epoch [163/200], Loss: 0.5183\n",
      "Epoch [163/200], Loss: 0.5346\n",
      "Epoch [163/200], Loss: 0.5800\n",
      "Epoch [163/200], Loss: 0.5710\n",
      "Epoch [164/200], Loss: 0.5498\n",
      "Epoch [164/200], Loss: 0.5648\n",
      "Epoch [164/200], Loss: 0.5515\n",
      "Epoch [164/200], Loss: 0.5411\n",
      "Epoch [165/200], Loss: 0.5436\n",
      "Epoch [165/200], Loss: 0.5402\n",
      "Epoch [165/200], Loss: 0.5793\n",
      "Epoch [165/200], Loss: 0.5542\n",
      "Epoch [166/200], Loss: 0.5477\n",
      "Epoch [166/200], Loss: 0.5713\n",
      "Epoch [166/200], Loss: 0.5652\n",
      "Epoch [166/200], Loss: 0.5436\n",
      "Epoch [167/200], Loss: 0.5498\n",
      "Epoch [167/200], Loss: 0.5623\n",
      "Epoch [167/200], Loss: 0.5543\n",
      "Epoch [167/200], Loss: 0.5639\n",
      "Epoch [168/200], Loss: 0.5722\n",
      "Epoch [168/200], Loss: 0.5698\n",
      "Epoch [168/200], Loss: 0.5300\n",
      "Epoch [168/200], Loss: 0.5716\n",
      "Epoch [169/200], Loss: 0.5649\n",
      "Epoch [169/200], Loss: 0.6150\n",
      "Epoch [169/200], Loss: 0.5717\n",
      "Epoch [169/200], Loss: 0.5676\n",
      "Epoch [170/200], Loss: 0.6158\n",
      "Epoch [170/200], Loss: 0.5799\n",
      "Epoch [170/200], Loss: 0.6214\n",
      "Epoch [170/200], Loss: 0.5451\n",
      "Epoch [171/200], Loss: 0.5669\n",
      "Epoch [171/200], Loss: 0.5557\n",
      "Epoch [171/200], Loss: 0.5542\n",
      "Epoch [171/200], Loss: 0.5534\n",
      "Epoch [172/200], Loss: 0.5656\n",
      "Epoch [172/200], Loss: 0.5867\n",
      "Epoch [172/200], Loss: 0.5644\n",
      "Epoch [172/200], Loss: 0.5632\n",
      "Epoch [173/200], Loss: 0.5434\n",
      "Epoch [173/200], Loss: 0.5412\n",
      "Epoch [173/200], Loss: 0.5874\n",
      "Epoch [173/200], Loss: 0.5537\n",
      "Epoch [174/200], Loss: 0.5791\n",
      "Epoch [174/200], Loss: 0.5235\n",
      "Epoch [174/200], Loss: 0.5581\n",
      "Epoch [174/200], Loss: 0.5505\n",
      "Epoch [175/200], Loss: 0.5238\n",
      "Epoch [175/200], Loss: 0.5804\n",
      "Epoch [175/200], Loss: 0.5671\n",
      "Epoch [175/200], Loss: 0.5470\n",
      "Epoch [176/200], Loss: 0.5657\n",
      "Epoch [176/200], Loss: 0.5432\n",
      "Epoch [176/200], Loss: 0.5568\n",
      "Epoch [176/200], Loss: 0.5489\n",
      "Epoch [177/200], Loss: 0.5613\n",
      "Epoch [177/200], Loss: 0.5342\n",
      "Epoch [177/200], Loss: 0.5546\n",
      "Epoch [177/200], Loss: 0.5566\n",
      "Epoch [178/200], Loss: 0.5739\n",
      "Epoch [178/200], Loss: 0.5439\n",
      "Epoch [178/200], Loss: 0.5287\n",
      "Epoch [178/200], Loss: 0.5531\n",
      "Epoch [179/200], Loss: 0.5499\n",
      "Epoch [179/200], Loss: 0.5710\n",
      "Epoch [179/200], Loss: 0.5419\n",
      "Epoch [179/200], Loss: 0.5583\n",
      "Epoch [180/200], Loss: 0.5732\n",
      "Epoch [180/200], Loss: 0.5460\n",
      "Epoch [180/200], Loss: 0.5541\n",
      "Epoch [180/200], Loss: 0.5486\n",
      "Epoch [181/200], Loss: 0.5647\n",
      "Epoch [181/200], Loss: 0.5705\n",
      "Epoch [181/200], Loss: 0.5377\n",
      "Epoch [181/200], Loss: 0.5478\n",
      "Epoch [182/200], Loss: 0.5536\n",
      "Epoch [182/200], Loss: 0.5699\n",
      "Epoch [182/200], Loss: 0.5719\n",
      "Epoch [182/200], Loss: 0.5549\n",
      "Epoch [183/200], Loss: 0.5866\n",
      "Epoch [183/200], Loss: 0.5944\n",
      "Epoch [183/200], Loss: 0.5652\n",
      "Epoch [183/200], Loss: 0.6102\n",
      "Epoch [184/200], Loss: 0.5438\n",
      "Epoch [184/200], Loss: 0.6459\n",
      "Epoch [184/200], Loss: 0.6295\n",
      "Epoch [184/200], Loss: 0.5818\n",
      "Epoch [185/200], Loss: 0.5702\n",
      "Epoch [185/200], Loss: 0.5742\n",
      "Epoch [185/200], Loss: 0.5522\n",
      "Epoch [185/200], Loss: 0.5795\n",
      "Epoch [186/200], Loss: 0.6067\n",
      "Epoch [186/200], Loss: 0.5827\n",
      "Epoch [186/200], Loss: 0.5184\n",
      "Epoch [186/200], Loss: 0.6157\n",
      "Epoch [187/200], Loss: 0.5351\n",
      "Epoch [187/200], Loss: 0.6068\n",
      "Epoch [187/200], Loss: 0.6299\n",
      "Epoch [187/200], Loss: 0.5487\n",
      "Epoch [188/200], Loss: 0.5596\n",
      "Epoch [188/200], Loss: 0.5771\n",
      "Epoch [188/200], Loss: 0.5502\n",
      "Epoch [188/200], Loss: 0.5875\n",
      "Epoch [189/200], Loss: 0.6273\n",
      "Epoch [189/200], Loss: 0.5460\n",
      "Epoch [189/200], Loss: 0.5832\n",
      "Epoch [189/200], Loss: 0.6047\n",
      "Epoch [190/200], Loss: 0.5184\n",
      "Epoch [190/200], Loss: 0.6035\n",
      "Epoch [190/200], Loss: 0.6047\n",
      "Epoch [190/200], Loss: 0.5808\n",
      "Epoch [191/200], Loss: 0.5550\n",
      "Epoch [191/200], Loss: 0.6180\n",
      "Epoch [191/200], Loss: 0.5346\n",
      "Epoch [191/200], Loss: 0.5638\n",
      "Epoch [192/200], Loss: 0.6207\n",
      "Epoch [192/200], Loss: 0.5624\n",
      "Epoch [192/200], Loss: 0.5711\n",
      "Epoch [192/200], Loss: 0.5586\n",
      "Epoch [193/200], Loss: 0.6101\n",
      "Epoch [193/200], Loss: 0.5657\n",
      "Epoch [193/200], Loss: 0.5767\n",
      "Epoch [193/200], Loss: 0.6207\n",
      "Epoch [194/200], Loss: 0.5854\n",
      "Epoch [194/200], Loss: 0.5780\n",
      "Epoch [194/200], Loss: 0.5360\n",
      "Epoch [194/200], Loss: 0.6262\n",
      "Epoch [195/200], Loss: 0.5290\n",
      "Epoch [195/200], Loss: 0.5610\n",
      "Epoch [195/200], Loss: 0.5995\n",
      "Epoch [195/200], Loss: 0.5631\n",
      "Epoch [196/200], Loss: 0.5482\n",
      "Epoch [196/200], Loss: 0.5582\n",
      "Epoch [196/200], Loss: 0.5390\n",
      "Epoch [196/200], Loss: 0.5816\n",
      "Epoch [197/200], Loss: 0.5785\n",
      "Epoch [197/200], Loss: 0.5439\n",
      "Epoch [197/200], Loss: 0.5394\n",
      "Epoch [197/200], Loss: 0.5605\n",
      "Epoch [198/200], Loss: 0.5464\n",
      "Epoch [198/200], Loss: 0.5442\n",
      "Epoch [198/200], Loss: 0.5346\n",
      "Epoch [198/200], Loss: 0.5912\n",
      "Epoch [199/200], Loss: 0.5331\n",
      "Epoch [199/200], Loss: 0.5570\n",
      "Epoch [199/200], Loss: 0.5649\n",
      "Epoch [199/200], Loss: 0.5535\n",
      "Epoch [200/200], Loss: 0.5116\n",
      "Epoch [200/200], Loss: 0.5945\n",
      "Epoch [200/200], Loss: 0.5801\n",
      "Epoch [200/200], Loss: 0.5537\n"
     ]
    }
   ],
   "source": [
    "train(model, batchsize = 500, num_epoch = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model, totnum = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
